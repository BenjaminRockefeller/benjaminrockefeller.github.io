---
layout: homepage
---

## About Me

Today’s language models speak fluently—but struggle to think.  
which hallucinate, fail to verify, and can’t explain their own answers.  

As a researcher in reasoning-centric LLMs at Nanyang Technological University,  
I work to close this cognitive gap—designing models that don’t just generate,  
but reason, justify, and self-correct.  

I’ve led the development of reasoning-driven LLM systems—  
from retrieval-augmented generation and causality-aware modeling  
to low-latency inference engines for verifiable, scalable deployment.  

My recent work focuses on closed-loop generation,  
where models iteratively refine outputs through embedded self-feedback.  

This includes **REFLEXION**, a Transformer framework for multi-step self-reflection,  
and **CAT**, a citation-aware tagging system that ensures factual grounding.  

Because the future of AI isn’t about more tokens—  
it’s about building models that know when they’re wrong, and know how to fix it.

---

## Research Interests

From fluency to trust: how do we build reasoning-capable LLMs?

Today’s models speak fluently, but without understanding.  
And that gap—between language and cognition—is where trust begins to break.  

My research tackles this challenge in two stages.

**First**, I focus on closing the hallucination gap—  
developing models that can detect when they’re wrong  
and revise themselves during inference.  

I’ve built systems like **REFLEXION**,  
which embeds feedback loops into Transformer generation  
and reduced hallucination by over 30% in reasoning benchmarks;  

**CAT**, a citation-aware tagging engine that grounds outputs  
in verifiable sources with traceable references;  

and **MEMOFORMER**, which extends reasoning across long contexts  
using memory persistence, improving coherence in multi-hop tasks.  

**The next stage** of my research explores  
how models can move from sequential prediction  
to nonlinear, associative reasoning—  
constructing meaning across ambiguity,  
grounded in memory and capable of abstract inference.  

Because the future of intelligence isn’t defined by how much it can say,  
but by whether it can reason, reflect, and earn trust.

---

{% include_relative _includes/publications.md %}
{% include_relative _includes/services.md %}
